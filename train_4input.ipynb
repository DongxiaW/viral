{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import re, random, pickle, glob, os, difflib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from utils import *\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root = 'train/'\n",
    "test_root = 'test/'\n",
    "model_root = 'models/'\n",
    "train_set = ['x10_reads.fa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_fasta(train_root + train_set[0])\n",
    "train_reads_original = np.array(seqs2onehot(np.array(df.seqs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reads = np.delete(train_reads_original,3,axis=2)\n",
    "# mask = train_reads_original[...,3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reads = np.expand_dims(train_reads, -1)\n",
    "# mask = np.invert(np.expand_dims(mask, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169538, 150, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_reads.shape)\n",
    "# print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.where(mask==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(169538, 150, 4, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        output = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = encoder(data)\n",
    "            reconstruction = decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            )\n",
    "            reconstruction_loss *= 150 * 4\n",
    "            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            kl_loss = tf.reduce_mean(kl_loss)\n",
    "            kl_loss *= -0.5\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\n",
    "            \"loss\": total_loss,\n",
    "            \"reconstruction_loss\": reconstruction_loss,\n",
    "            \"kl_loss\": kl_loss,\n",
    "        }\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 10\n",
    "width = 32\n",
    "input_size = (150,4,1)\n",
    "filter_size = (10, 4) #10, 15, 20\n",
    "epochs = 50\n",
    "batch_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_num = 'input_d4'\n",
    "ckpt_dir = os.path.join(model_root, ckpt_num, '')\n",
    "if (os.path.isdir(ckpt_dir) == False):\n",
    "    os.mkdir(os.path.join(ckpt_dir, ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 150, 4, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 75, 4, 32)    1312        input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 38, 4, 32)    40992       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 4864)         0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 32)           155680      flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 10)           330         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 10)           330         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sampling_3 (Sampling)           (None, 10)           0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 198,644\n",
      "Trainable params: 198,644\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4864)              53504     \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 38, 4, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 76, 4, 32)         40992     \n",
      "_________________________________________________________________\n",
      "cropping2d_3 (Cropping2D)    (None, 75, 4, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 150, 4, 32)        40992     \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 150, 4, 1)         1281      \n",
      "=================================================================\n",
      "Total params: 136,769\n",
      "Trainable params: 136,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    encoder_inputs = keras.Input(shape=input_size)\n",
    "    x = layers.Conv2D(width, filter_size, activation=\"relu\", strides=(2,1), padding=\"same\")(encoder_inputs)\n",
    "    x = layers.Conv2D(width, filter_size, activation=\"relu\", strides=(2,1), padding=\"same\")(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(width, activation=\"relu\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    print(encoder.summary())\n",
    "    \n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(38 * 4 * width, activation=\"relu\")(latent_inputs)\n",
    "    x = layers.Reshape((38, 4, width))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(width, filter_size, activation=\"relu\", strides=(2, 1), padding=\"same\")(x)\n",
    "    x = layers.Cropping2D(cropping=((0, 1), (0, 0)))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(width, filter_size, activation=\"relu\", strides=(2, 1), padding=\"same\")(x)\n",
    "    decoder_outputs = layers.Conv2D(1, filter_size, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    print(decoder.summary())\n",
    "    \n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 360.8458 - reconstruction_loss: 359.8937 - kl_loss: 0.9521\n",
      "Epoch 2/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 338.6883 - reconstruction_loss: 338.2589 - kl_loss: 0.4294\n",
      "Epoch 3/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 337.6677 - reconstruction_loss: 337.3494 - kl_loss: 0.3183\n",
      "Epoch 4/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 336.9968 - reconstruction_loss: 336.7448 - kl_loss: 0.2520\n",
      "Epoch 5/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 336.3723 - reconstruction_loss: 336.2154 - kl_loss: 0.1569\n",
      "Epoch 6/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.8443 - reconstruction_loss: 335.7608 - kl_loss: 0.0835\n",
      "Epoch 7/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.5591 - reconstruction_loss: 335.5147 - kl_loss: 0.0443\n",
      "Epoch 8/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4579 - reconstruction_loss: 335.4311 - kl_loss: 0.0269\n",
      "Epoch 9/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4647 - reconstruction_loss: 335.4472 - kl_loss: 0.0175\n",
      "Epoch 10/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4048 - reconstruction_loss: 335.3930 - kl_loss: 0.0118\n",
      "Epoch 11/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4213 - reconstruction_loss: 335.4132 - kl_loss: 0.0081\n",
      "Epoch 12/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 335.4046 - reconstruction_loss: 335.3990 - kl_loss: 0.0057\n",
      "Epoch 13/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.3924 - reconstruction_loss: 335.3883 - kl_loss: 0.0040\n",
      "Epoch 14/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 335.3811 - reconstruction_loss: 335.3782 - kl_loss: 0.0030\n",
      "Epoch 15/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.3757 - reconstruction_loss: 335.3735 - kl_loss: 0.0022\n",
      "Epoch 16/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.3663 - reconstruction_loss: 335.3643 - kl_loss: 0.0020\n",
      "Epoch 17/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4028 - reconstruction_loss: 335.3996 - kl_loss: 0.0032\n",
      "Epoch 18/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.3564 - reconstruction_loss: 335.3462 - kl_loss: 0.0102\n",
      "Epoch 19/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 334.9476 - reconstruction_loss: 334.8741 - kl_loss: 0.0734\n",
      "Epoch 20/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 333.7593 - reconstruction_loss: 333.4515 - kl_loss: 0.3078\n",
      "Epoch 21/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 333.4599 - reconstruction_loss: 333.1198 - kl_loss: 0.3401\n",
      "Epoch 22/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 333.1154 - reconstruction_loss: 332.7863 - kl_loss: 0.3290\n",
      "Epoch 23/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 332.0092 - reconstruction_loss: 331.5141 - kl_loss: 0.4951\n",
      "Epoch 24/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 330.8060 - reconstruction_loss: 330.0305 - kl_loss: 0.7755\n",
      "Epoch 25/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 329.5647 - reconstruction_loss: 328.4372 - kl_loss: 1.1275\n",
      "Epoch 26/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.1312 - reconstruction_loss: 326.6223 - kl_loss: 1.5089\n",
      "Epoch 27/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 327.2209 - reconstruction_loss: 325.5615 - kl_loss: 1.6593\n",
      "Epoch 28/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 326.8685 - reconstruction_loss: 325.2143 - kl_loss: 1.6542\n",
      "Epoch 29/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 326.7332 - reconstruction_loss: 325.1267 - kl_loss: 1.6065\n",
      "Epoch 30/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.5545 - reconstruction_loss: 324.9740 - kl_loss: 1.5805\n",
      "Epoch 31/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 326.4658 - reconstruction_loss: 324.9026 - kl_loss: 1.5632\n",
      "Epoch 32/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.4094 - reconstruction_loss: 324.8588 - kl_loss: 1.5506\n",
      "Epoch 33/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 326.4681 - reconstruction_loss: 324.9295 - kl_loss: 1.5387\n",
      "Epoch 34/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.3533 - reconstruction_loss: 324.8158 - kl_loss: 1.5375\n",
      "Epoch 35/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 326.3287 - reconstruction_loss: 324.7948 - kl_loss: 1.5339\n",
      "Epoch 36/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.2230 - reconstruction_loss: 324.6898 - kl_loss: 1.5332\n",
      "Epoch 37/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 326.2021 - reconstruction_loss: 324.6718 - kl_loss: 1.5303\n",
      "Epoch 38/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.1110 - reconstruction_loss: 324.5811 - kl_loss: 1.5299\n",
      "Epoch 39/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.1853 - reconstruction_loss: 324.6566 - kl_loss: 1.5287\n",
      "Epoch 40/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.1741 - reconstruction_loss: 324.6386 - kl_loss: 1.5354\n",
      "Epoch 41/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 326.0714 - reconstruction_loss: 324.4989 - kl_loss: 1.5725\n",
      "Epoch 42/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.7760 - reconstruction_loss: 324.1083 - kl_loss: 1.6677\n",
      "Epoch 43/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.5929 - reconstruction_loss: 323.9043 - kl_loss: 1.6885\n",
      "Epoch 44/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.5316 - reconstruction_loss: 323.8529 - kl_loss: 1.6787\n",
      "Epoch 45/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.5262 - reconstruction_loss: 323.8561 - kl_loss: 1.6701\n",
      "Epoch 46/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.4136 - reconstruction_loss: 323.7481 - kl_loss: 1.6655\n",
      "Epoch 47/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.4712 - reconstruction_loss: 323.8122 - kl_loss: 1.6590\n",
      "Epoch 48/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.4438 - reconstruction_loss: 323.7851 - kl_loss: 1.6587\n",
      "Epoch 49/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.4462 - reconstruction_loss: 323.7927 - kl_loss: 1.6535\n",
      "Epoch 50/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 325.3757 - reconstruction_loss: 323.7232 - kl_loss: 1.6525\n"
     ]
    }
   ],
   "source": [
    "# lr 0.0001\n",
    "history = vae.fit(train_reads, epochs=epochs, batch_size=batch_size, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 343.6486 - reconstruction_loss: 343.6481 - kl_loss: 5.2490e-04\n",
      "Epoch 2/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 336.2361 - reconstruction_loss: 336.2351 - kl_loss: 0.0010\n",
      "Epoch 3/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.5576 - reconstruction_loss: 335.5339 - kl_loss: 0.0237\n",
      "Epoch 4/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 332.6154 - reconstruction_loss: 332.0263 - kl_loss: 0.5890\n",
      "Epoch 5/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 330.0693 - reconstruction_loss: 329.1157 - kl_loss: 0.9536\n",
      "Epoch 6/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.6144 - reconstruction_loss: 327.4041 - kl_loss: 1.2103\n",
      "Epoch 7/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.4005 - reconstruction_loss: 327.2596 - kl_loss: 1.1409\n",
      "Epoch 8/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.2872 - reconstruction_loss: 327.1681 - kl_loss: 1.1192\n",
      "Epoch 9/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.2394 - reconstruction_loss: 327.1346 - kl_loss: 1.1048\n",
      "Epoch 10/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.2012 - reconstruction_loss: 327.1109 - kl_loss: 1.0903\n",
      "Epoch 11/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.1645 - reconstruction_loss: 327.0813 - kl_loss: 1.0832\n",
      "Epoch 12/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.1156 - reconstruction_loss: 327.0426 - kl_loss: 1.0730\n",
      "Epoch 13/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.0591 - reconstruction_loss: 326.9927 - kl_loss: 1.0664\n",
      "Epoch 14/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.1295 - reconstruction_loss: 327.0717 - kl_loss: 1.0578\n",
      "Epoch 15/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.0796 - reconstruction_loss: 327.0263 - kl_loss: 1.0533\n",
      "Epoch 16/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.0179 - reconstruction_loss: 326.9707 - kl_loss: 1.0472\n",
      "Epoch 17/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.9735 - reconstruction_loss: 326.9268 - kl_loss: 1.0467\n",
      "Epoch 18/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.9306 - reconstruction_loss: 326.8839 - kl_loss: 1.0466\n",
      "Epoch 19/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 327.9694 - reconstruction_loss: 326.9266 - kl_loss: 1.0428\n",
      "Epoch 20/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.9470 - reconstruction_loss: 326.9052 - kl_loss: 1.0418\n",
      "Epoch 21/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.9716 - reconstruction_loss: 326.9318 - kl_loss: 1.0398\n",
      "Epoch 22/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 327.9459 - reconstruction_loss: 326.9051 - kl_loss: 1.0408\n",
      "Epoch 23/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 328.0128 - reconstruction_loss: 326.9746 - kl_loss: 1.0383\n",
      "Epoch 24/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.8984 - reconstruction_loss: 326.8544 - kl_loss: 1.0441\n",
      "Epoch 25/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.9324 - reconstruction_loss: 326.8892 - kl_loss: 1.0432\n",
      "Epoch 26/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 327.8458 - reconstruction_loss: 326.7992 - kl_loss: 1.0466\n",
      "Epoch 27/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.9359 - reconstruction_loss: 326.8885 - kl_loss: 1.0474\n",
      "Epoch 28/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.8188 - reconstruction_loss: 326.7674 - kl_loss: 1.0513\n",
      "Epoch 29/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.8543 - reconstruction_loss: 326.7997 - kl_loss: 1.0546\n",
      "Epoch 30/50\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 327.8208 - reconstruction_loss: 326.7624 - kl_loss: 1.0584\n",
      "Epoch 31/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.7747 - reconstruction_loss: 326.7114 - kl_loss: 1.0634\n",
      "Epoch 32/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.7537 - reconstruction_loss: 326.6846 - kl_loss: 1.0692\n",
      "Epoch 33/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.7510 - reconstruction_loss: 326.6800 - kl_loss: 1.0709\n",
      "Epoch 34/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.7562 - reconstruction_loss: 326.6815 - kl_loss: 1.0747\n",
      "Epoch 35/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 327.7012 - reconstruction_loss: 326.6286 - kl_loss: 1.0726\n",
      "Epoch 36/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 325.8687 - reconstruction_loss: 324.7735 - kl_loss: 1.0951\n",
      "Epoch 37/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 325.1872 - reconstruction_loss: 324.0762 - kl_loss: 1.1110\n",
      "Epoch 38/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 325.0024 - reconstruction_loss: 323.8818 - kl_loss: 1.1206\n",
      "Epoch 39/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.8691 - reconstruction_loss: 323.7430 - kl_loss: 1.1261\n",
      "Epoch 40/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.8799 - reconstruction_loss: 323.7508 - kl_loss: 1.1290\n",
      "Epoch 41/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.8333 - reconstruction_loss: 323.6992 - kl_loss: 1.1341\n",
      "Epoch 42/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.7605 - reconstruction_loss: 323.6235 - kl_loss: 1.1370\n",
      "Epoch 43/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.6900 - reconstruction_loss: 323.5494 - kl_loss: 1.1406\n",
      "Epoch 44/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.7242 - reconstruction_loss: 323.5812 - kl_loss: 1.1430\n",
      "Epoch 45/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.6847 - reconstruction_loss: 323.5404 - kl_loss: 1.1443\n",
      "Epoch 46/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.6762 - reconstruction_loss: 323.5327 - kl_loss: 1.1435\n",
      "Epoch 47/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.6660 - reconstruction_loss: 323.5166 - kl_loss: 1.1494\n",
      "Epoch 48/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.6750 - reconstruction_loss: 323.5248 - kl_loss: 1.1502\n",
      "Epoch 49/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.6453 - reconstruction_loss: 323.4914 - kl_loss: 1.1539\n",
      "Epoch 50/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 324.6451 - reconstruction_loss: 323.4904 - kl_loss: 1.1546\n"
     ]
    }
   ],
   "source": [
    "# lr 0.001\n",
    "history = vae.fit(train_reads, epochs=epochs, batch_size=batch_size, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:batch_all_reduce: 18 all-reduces with algorithm = nccl, num_packs = 1\n",
      "83/83 [==============================] - 3s 41ms/step - loss: 357.5218 - reconstruction_loss: 357.5015 - kl_loss: 0.0202\n",
      "Epoch 2/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.5530 - reconstruction_loss: 335.5514 - kl_loss: 0.0016\n",
      "Epoch 3/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.5545 - reconstruction_loss: 335.5533 - kl_loss: 0.0011\n",
      "Epoch 4/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.5302 - reconstruction_loss: 335.5294 - kl_loss: 7.9347e-04\n",
      "Epoch 5/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.5134 - reconstruction_loss: 335.5129 - kl_loss: 5.3578e-04\n",
      "Epoch 6/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4750 - reconstruction_loss: 335.4746 - kl_loss: 3.5575e-04\n",
      "Epoch 7/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4263 - reconstruction_loss: 335.4261 - kl_loss: 2.3407e-04\n",
      "Epoch 8/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4398 - reconstruction_loss: 335.4396 - kl_loss: 1.5335e-04\n",
      "Epoch 9/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4095 - reconstruction_loss: 335.4094 - kl_loss: 1.0028e-04\n",
      "Epoch 10/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4346 - reconstruction_loss: 335.4345 - kl_loss: 6.5398e-05\n",
      "Epoch 11/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.4187 - reconstruction_loss: 335.4187 - kl_loss: 4.2415e-05\n",
      "Epoch 12/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.3703 - reconstruction_loss: 335.3703 - kl_loss: 2.7245e-05\n",
      "Epoch 13/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.3870 - reconstruction_loss: 335.3870 - kl_loss: 1.7258e-05\n",
      "Epoch 14/50\n",
      "83/83 [==============================] - 3s 40ms/step - loss: 335.3851 - reconstruction_loss: 335.3851 - kl_loss: 1.0725e-05\n",
      "Epoch 15/50\n",
      "49/83 [================>.............] - ETA: 1s - loss: 335.4049 - reconstruction_loss: 335.4049 - kl_loss: 7.2123e-06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-0eff27abd2d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lr 0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_reads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/conda/envs/ecseg/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr 0.01\n",
    "history = vae.fit(train_reads, epochs=epochs, batch_size=batch_size, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 2s 20ms/step - loss: 338.3851 - reconstruction_loss: 338.3850 - kl_loss: 1.0581e-04\n"
     ]
    }
   ],
   "source": [
    "history = vae.fit(train_reads, epochs=epochs, batch_size=batch_size, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reads[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('total loss')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['reconstruction_loss'])\n",
    "plt.title('reconstruction loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['kl_loss'])\n",
    "plt.title('kl loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save('models/ckpt1/encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sets = ['test_easy_1.fa', 'test_easy_2.fa', 'test_intermidate_1.fa', 'test_intermidate_2.fa', 'test_difficult_1.fa', \n",
    "#              'test_difficult_2.fa', 'test_very_difficult_1.fa', 'test_very_difficult_2.fa']\n",
    "# easy_1 = assign_labels(read_fasta(test_root + test_sets[0]))\n",
    "# easy_2 = assign_labels(read_fasta(test_root + test_sets[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy_1_predictions = encoder.predict(train_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.expand_dims(np.array(seqs2onehot([easy_1.loc[105487]['seqs']])), -1)\n",
    "# b = encoder.predict(a)\n",
    "# c = decoder.predict(b[-1]); c = np.squeeze(c)\n",
    "# difflib.SequenceMatcher(None, np.argmax((c>0.01), axis=1), np.argmax(np.squeeze(a), axis=1)).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 16)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array(b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
