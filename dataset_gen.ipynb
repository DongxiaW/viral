{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import os\n",
    "from keras.layers import Dense, Flatten, Conv2D, Conv1D, MaxPooling2D, Embedding, Input, Dropout, Reshape, Activation\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import multi_gpu_model \n",
    "import matplotlib.pylab as plt\n",
    "from skimage.transform import resize\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from tqdm import tnrange\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data): #transform sequence into one hot encoding\n",
    "    def one_hot_encode(seq):\n",
    "        mapping = dict(zip(\"acgnt\", range(5)))     \n",
    "        seq2 = [mapping[i] if i in ['a', 't', 'c', 'g', 'n'] else mapping['n'] for i in seq]\n",
    "        return np.eye(5)[seq2]\n",
    "    read_size = 150\n",
    "    seq = one_hot_encode(data[0])\n",
    "    seq = np.expand_dims(seq, axis = -1)\n",
    "    return seq\n",
    "\n",
    "def read_data(root_dir, file):\n",
    "    viral = []\n",
    "    hum = []\n",
    "    file_path = os.path.join(root_dir, file)\n",
    "    for seq_record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        if('v' in seq_record.id):\n",
    "            viral.append(seq_record.seq)\n",
    "        if('chr1' in seq_record.id):\n",
    "            hum.append(seq_record.seq)\n",
    "    return viral, hum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/home/ecvol/data/viral/'\n",
    "read_length=150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training reads from viral sequences\n",
    "\n",
    "Get 5000 reads from each viral reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 337/337 [00:58<00:00,  5.75it/s]\n"
     ]
    }
   ],
   "source": [
    "train_seq= []\n",
    "for idx, seq in enumerate(SeqIO.parse(root_dir + \"hpv.fas\", \"fasta\")):\n",
    "    sequence = str(seq.seq)\n",
    "    if(not sequence.islower()):\n",
    "        seq = sequence.lower()\n",
    "    train_seq.append(sequence)\n",
    "\n",
    "############################################################################################\n",
    "smallest_len = min(len(i) for i in train_seq)\n",
    "num_reads = 5000\n",
    "rand_reads = np.random.choice(range(smallest_len-152), size = num_reads, replace=False)\n",
    "train_set = np.empty([len(train_seq)*num_reads,read_length, 5, 1], np.int8)\n",
    "train_id = 0\n",
    "for idx, seq in tqdm(enumerate(train_seq), total = len(train_seq)):\n",
    "    for read_start in rand_reads:\n",
    "        read = np.array(seq[read_start:read_start+read_length])\n",
    "        train_set[train_id] = transform(np.expand_dims(read, axis=0))\n",
    "        train_id += 1\n",
    "############################################################################################\n",
    "np.save(root_dir + 'v_ref_reads', train_set) #when loading, call np.random.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1685000, 150, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "#print(read, train_set[-1]) #check\n",
    "print(train_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get training and test reads from chr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2685000/2685000 [00:03<00:00, 811283.27it/s]\n",
      "100%|██████████| 2685000/2685000 [03:14<00:00, 13819.38it/s]\n"
     ]
    }
   ],
   "source": [
    "for idx, seq in enumerate(SeqIO.parse(root_dir+\"hg19full.fa\", \"fasta\")):\n",
    "    train_seq = str(seq.seq)\n",
    "    if(not train_seq.islower()):\n",
    "        train_seq = train_seq.lower()\n",
    "    break\n",
    "\n",
    "############################################################################################\n",
    "num_reads = train_set.shape[0] + 1000000\n",
    "smallest_len = len(train_seq)\n",
    "rand_reads = np.random.choice(range(smallest_len-152), size = num_reads, replace=False)\n",
    "myfile = open(root_dir+'text.txt', 'w')\n",
    "for idx, read_start in tqdm(enumerate(rand_reads), total=num_reads):\n",
    "    read = str(train_seq[read_start:read_start+read_length]) + '\\n'\n",
    "    myfile.write(read)\n",
    "myfile.close()\n",
    "\n",
    "############################################################################################\n",
    "with open(root_dir+'text.txt') as f:\n",
    "    content = f.readlines()\n",
    "content = [x.strip().lower() for x in content] \n",
    "\n",
    "############################################################################################\n",
    "train_set = np.empty([len(content), read_length, 5, 1], np.int8)\n",
    "for idx, contents in tqdm(enumerate(content), total=len(content)):\n",
    "    train_set[idx] = transform(np.expand_dims((np.array(list(contents))), axis=0))\n",
    "    \n",
    "np.save(root_dir+'chr1_ref_reads', train_set) #already shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test reads from viral sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "viral_seq = 0\n",
    "for idx, seq in enumerate(SeqIO.parse(root_dir+\"agpv1.fa\", \"fasta\")):\n",
    "    viral_seq = seq.seq\n",
    "    \n",
    "train_seq = viral_seq\n",
    "num_reads = 10000\n",
    "smallest_len = len(train_seq)\n",
    "rand_reads = np.random.choice(range(smallest_len-152), size = num_reads, replace=False)\n",
    "myfile = open('text.txt', 'w')\n",
    "for idx, read_start in tqdm(enumerate(rand_reads), total=num_reads):\n",
    "    read = str(train_seq[read_start:read_start+read_length]) + '\\n'\n",
    "    myfile.write(read)\n",
    "myfile.close()\n",
    "with open('text.txt') as f:\n",
    "    content = f.readlines()\n",
    "content = [x.strip().lower() for x in content] \n",
    "train_set = np.empty([len(content), read_length, 5, 1], np.int8)\n",
    "for idx, contents in tqdm(enumerate(content), total=len(content)):\n",
    "    train_set[idx] = transform(np.expand_dims((np.array(list(contents))), axis=0))\n",
    "\n",
    "############################################################################################\n",
    "np.save('./data/v_ref_reads_test', train_set) #already shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test reads from viral sequence (using nam's reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 13590.70it/s]\n"
     ]
    }
   ],
   "source": [
    "viral_seq = []\n",
    "for idx, seq in enumerate(SeqIO.parse(root_dir+\"run_1.150.0.000000.fas\", \"fasta\")):\n",
    "    if(seq.id[0]=='v'):\n",
    "        viral_seq.append(str(seq.seq))\n",
    "        if(not viral_seq[-1].islower()):\n",
    "            viral_seq[-1] = viral_seq[-1].lower()\n",
    "\n",
    "train_set = np.empty([len(viral_seq), read_length, 5, 1], np.int8)\n",
    "for idx, contents in tqdm(enumerate(viral_seq), total=len(viral_seq)):\n",
    "    train_set[idx] = transform(np.expand_dims((np.array(list(contents))), axis=0))\n",
    "\n",
    "############################################################################################\n",
    "np.save(root_dir+'v_ref_reads_test', train_set) #already shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(contents, train_set[-1]) #check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test and training set\n",
    "\n",
    "#### 1 = virus, 0 human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (3370000, 150, 5, 1) Y_train shape:  (3370000,) X_test.shape (1010000, 150, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "v_ref_reads = np.load(root_dir + 'v_ref_reads.npy')\n",
    "np.random.shuffle(v_ref_reads)\n",
    "h_ref_reads = np.load(root_dir + 'chr1_ref_reads.npy')\n",
    "\n",
    "hum_train = h_ref_reads[:v_ref_reads.shape[0]]\n",
    "X_train = np.array([*v_ref_reads, *hum_train])\n",
    "Y_train = np.array([*np.ones(v_ref_reads.shape[0]), *np.zeros(hum_train.shape[0])])\n",
    "np.save(root_dir+'train_set.npy', X_train)\n",
    "np.save(root_dir+'train_set_label.npy', Y_train)\n",
    "\n",
    "hum_test = h_ref_reads[v_ref_reads.shape[0]:]\n",
    "viral_test = np.load(root_dir + 'v_ref_reads_test.npy')\n",
    "X_test = np.concatenate((viral_test, hum_test), axis=0) # 1 = virus, 0 human\n",
    "Y_test = np.array([*np.ones(viral_test.shape[0]), *np.zeros(hum_test.shape[0])])\n",
    "np.save(root_dir+'test_set.npy', X_test)\n",
    "np.save(root_dir+'test_set_label.npy', Y_test)\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape, \"Y_train shape: \", Y_train.shape, \"X_test.shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training set for integrated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vreads = v_ref_reads.shape[0]\n",
    "num_hreads = hum_train.shape[0]\n",
    "X2_train = np.empty([1000000, read_length, 5, 1], dtype=np.dtype(bool))\n",
    "Y2_train = np.empty([1000000,read_length], dtype=np.dtype(bool))\n",
    "num_train = 1000000\n",
    "pos = np.random.choice(range(read_length-1), size= num_train)\n",
    "vreads = np.random.choice(range(num_vreads), size= num_train, replace=False)\n",
    "hreads = np.random.choice(range(num_hreads), size= num_train, replace=False)\n",
    "p_hstart = np.random.randint(low = 0, high = 2, size=num_train) #probability that the read starts with human seq\n",
    "for i in range(num_train):\n",
    "    vread = v_ref_reads[vreads[i]]\n",
    "    hread = hum_train[hreads[i]]\n",
    "    GT = np.zeros(150)\n",
    "    if(p_hstart[i]):\n",
    "        X2_train[i] = np.vstack((hread[:pos[i]], vread[pos[i]:]))\n",
    "        GT[pos[i]:] = 1\n",
    "    else:\n",
    "        X2_train[i] = np.vstack((vread[:pos[i]], hread[pos[i]:]))\n",
    "        GT[:pos[i]] = 1\n",
    "    Y2_train[i] = GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(root_dir+'integ_train_set.npy', X2_train)\n",
    "np.save(root_dir+'integ_train_set_label.npy', Y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vread[pos[i]], ' --', hread[pos[i]-2:pos[i]], ' --', X2_train[i][pos[i]-2:pos[i]+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test set for integrated sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vreads = viral_test.shape[0]\n",
    "num_hreads = hum_test.shape[0]\n",
    "X2_test = np.empty([1000000, read_length, 5, 1], dtype=np.dtype(bool))\n",
    "Y2_test = np.empty([1000000,read_length], dtype=np.dtype(bool))\n",
    "num_test = 1000000\n",
    "pos = np.random.choice(range(read_length-1), size= num_train)\n",
    "vreads = np.random.choice(range(num_vreads), size= num_test)\n",
    "hreads = np.random.choice(range(num_hreads), size= num_test)\n",
    "p_hstart = np.random.randint(low = 0, high = 2, size=num_train) #probability that the read starts with human seq\n",
    "for i in range(num_train):\n",
    "    vread = viral_test[vreads[i]]\n",
    "    hread = hum_test[hreads[i]]\n",
    "    GT = np.zeros(150)\n",
    "    if(p_hstart[i]):\n",
    "        X2_test[i] = np.vstack((hread[:pos[i]], vread[pos[i]:]))\n",
    "        GT[pos[i]:] = 1\n",
    "    else:\n",
    "        X2_test[i] = np.vstack((vread[:pos[i]], hread[pos[i]:]))\n",
    "        GT[:pos[i]] = 1\n",
    "    Y2_test[i] = GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(root_dir+'integ_test_set.npy', X2_test)\n",
    "np.save(root_dir+'integ_test_set_label.npy', Y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate reads from all chromosomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:01<00:00, 545893.08it/s]\n",
      "100%|██████████| 1000000/1000000 [01:13<00:00, 13619.35it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 563910.16it/s]\n",
      "100%|██████████| 1000000/1000000 [01:12<00:00, 13797.43it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 552547.49it/s]\n",
      "100%|██████████| 1000000/1000000 [01:11<00:00, 13911.85it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 553335.78it/s]\n",
      "100%|██████████| 1000000/1000000 [01:11<00:00, 13890.69it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 557262.33it/s]\n",
      "100%|██████████| 1000000/1000000 [01:11<00:00, 13946.25it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 560490.73it/s]\n",
      "100%|██████████| 1000000/1000000 [01:12<00:00, 13870.70it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 557926.13it/s]\n",
      "100%|██████████| 1000000/1000000 [01:12<00:00, 13853.11it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 555389.10it/s]\n",
      "100%|██████████| 1000000/1000000 [01:13<00:00, 13560.90it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 551359.91it/s]\n",
      "100%|██████████| 1000000/1000000 [01:11<00:00, 13953.27it/s]\n",
      "100%|██████████| 1000000/1000000 [00:01<00:00, 569374.26it/s]\n",
      " 37%|███▋      | 372340/1000000 [00:26<00:44, 14055.74it/s]"
     ]
    }
   ],
   "source": [
    "num_reads = 1000000\n",
    "\n",
    "for idx, seq in enumerate(SeqIO.parse(root_dir+\"hg19full.fa\", \"fasta\")):\n",
    "    train_seq = seq.seq\n",
    "    name = seq.id\n",
    "    seq_len = len(train_seq)\n",
    "    \n",
    "    if(len(train_seq) < num_reads):\n",
    "        num_reads = seq_len-151\n",
    "        \n",
    "    rand_reads = np.random.choice(range(num_reads), size = num_reads, replace=False)\n",
    "    myfile = open(root_dir+name+'.txt', 'w')\n",
    "    for idx, read_start in tqdm(enumerate(rand_reads), total=num_reads):\n",
    "        read = str(train_seq[read_start:read_start+read_length]) + '\\n'\n",
    "        myfile.write(read)\n",
    "    myfile.close()\n",
    "    with open(root_dir+name+'.txt') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip().lower() for x in content] \n",
    "\n",
    "    train_set = np.empty([len(content), read_length, 5, 1], np.int8)\n",
    "    for idx, contents in tqdm(enumerate(content), total=len(content)):\n",
    "        train_set[idx] = transform(np.expand_dims((np.array(list(contents))), axis=0))\n",
    "    \n",
    "    np.save(root_dir+name, train_set) #already shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "all_chr = np.empty([1, 150, 5, 1], np.bool)\n",
    "for file in glob.glob(root_dir+\"/chr_reads/\"+\"*.npy\"):\n",
    "    temp = np.load(file)\n",
    "    all_chr = np.vstack((all_chr, temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24016421, 150, 5, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chr.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
